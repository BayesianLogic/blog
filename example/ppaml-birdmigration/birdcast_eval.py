#!/usr/bin/python

# birdcast_eval.py
# Copyright (C) 2014 Oregon State University
# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in
#    the documentation and/or other materials provided with the distribution.
# 3. Neither Oregon State University's name nor the names of other contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY OREGON STATE UNIVERSITY AND OTHER CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING,
# BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
# IN NO EVENT SHALL OREGON STATE UNIVERSITY OR OTHER CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
# STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


# birdcast_eval.py -- evaluates Birdcast data set in relation to the ground truth
# version 0.70
__author__ = 'sorower'
#usage: birdcast_eval.py [-h] [--dataset DATASET] cps_output_dir ground_truth_dir eval_output_dir

#positional arguments:
#  cps_output_dir    output directory containing the program output (for each dataset in a subdirectory)
#                    to be evaluated against the ground truth
#  ground_truth_dir  ground truth directory containing the ground truth for each dataset in a subdirectory
#  eval_output_dir   evaluation output directory where the evaluation report
#                    files generated by this program need to be stored

# optional arguments:
#  -h, --help         show this help message and exit
#  --dataset DATASET  comma separated names of the datasets (d1,d2,d3) that need to be evaluated (default: all)

# example run: python birdcast_eval.py data\output\ data\ground\ results


version = '0.70'

import sys
import os
import time
from numpy import *
import pandas as pd
import argparse

import os.path as osp
################################ Defining Constants #################################


d1_output_param_path = osp.join('dataset1', 'estimated-parameters.csv')
d1_ground_param_path = osp.join('dataset1', 'ground-parameters.csv')

d2_output_reconstruction_path = osp.join('dataset2', '10x10x1000-train-reconstruction.csv')
d2_ground_reconstruction_path = osp.join('dataset2', '10x10x1000-reconstruction-ground.csv')
d2_output_prediction_path = osp.join('dataset2', '10x10x1000-test-prediction.csv')
d2_ground_prediction_path = osp.join('dataset2', '10x10x1000-prediction-ground.csv')
d2_output_param_path = osp.join('dataset2', 'estimated-parameters.csv')
d2_ground_param_path = osp.join('dataset2', 'ground-parameters.csv')

d3_output_reconstruction_path = osp.join('dataset3', '10x10x1000000-train-reconstruction.csv')
d3_ground_reconstruction_path = osp.join('dataset3', '10x10x1000000-reconstruction-ground.csv')
d3_output_prediction_path = osp.join('dataset3', '10x10x1000000-test-prediction.csv')
d3_ground_prediction_path = osp.join('dataset3', '10x10x1000000-prediction-ground.csv')
d3_output_param_path = osp.join('dataset3', 'estimated-parameters.csv')
d3_ground_param_path = osp.join('dataset3', 'ground-parameters.csv')

################################ Reading data #################################

# reads the reconstruction csv file. this csv file has the columns: year, day, from.cell, to.cell, number.of.birds.
def read_reconstruction_csv(path):
    if(osp.isfile(path) == False):
        return None
    reconstruction_csv = pd.read_csv(path)
    reconstruction_csv.columns = ['year','day','fromcell','tocell','number.of.birds']
    reconstruction_csv=reconstruction_csv.sort(columns=['year','day','fromcell','tocell'])
    return reconstruction_csv['number.of.birds']

# reads the prediction csv file. this csv file has the columns: year, day, from.cell, to.cell, number.birds, number.birds2.
def read_prediction_csv(path):
    if(osp.isfile(path) == False):
        return None, None
    prediction_csv = pd.read_csv(path)
    prediction_csv.columns = ['year','day','fromcell','tocell','number.birds','number.birds2']
    prediction_csv=prediction_csv.sort(columns=['year','day','fromcell','tocell'])

    # the last day we don't make a prediction for the night separating day t+1 and t+2. So we set -1 for the last day in the prediction table,
    # and the ground truth for the last column should also be -1 and then the squared difference for these values will be 0.
    # The value '-1' is used as it is an invalid number for prediction purposes.
    prediction_csv.loc[(prediction_csv['day']==19),'number.birds2'] = -1

    return prediction_csv['number.birds'], prediction_csv['number.birds2']

# reads the parameter csv file. this file has the following columns: b1,b2,b3,b4
# and just one row
def read_parameter_csv(path):
    if(osp.isfile(path) == False):
        return None
    parameter_csv = pd.read_csv(path)
    parameter_csv_list = list(map(list, parameter_csv.values))
    b = parameter_csv_list[0]
    return b

def read_output_dataset1(path):
    output_b = read_parameter_csv(osp.join(path,  d1_output_param_path))
    return output_b

def read_ground_dataset1(path):
    ground_b = read_parameter_csv(osp.join(path,  d1_ground_param_path))
    return ground_b

def read_output_dataset2(path):
    output_reconstruction = read_reconstruction_csv(osp.join(path,  d2_output_reconstruction_path))
    if output_reconstruction is None:
        print('CPS reconstruction output file missing for Dataset2: ' + path + d2_output_reconstruction_path)

    output_prediction1, output_prediction2 = read_prediction_csv(osp.join(path,  d2_output_prediction_path))
    if output_prediction1 is None or output_prediction2 is None:
        print('CPS prediction output file missing for Dataset2: ' + path + d2_output_prediction_path)

    output_b = read_parameter_csv(osp.join(path,  d2_output_param_path))
    if output_b is None:
        print('CPS parameter output file missing for Dataset2: ' + path + d2_output_param_path)

    return output_reconstruction, output_prediction1, output_prediction2, output_b

def read_ground_dataset2(path):
    ground_reconstruction = read_reconstruction_csv(osp.join(path,  d2_ground_reconstruction_path))
    if ground_reconstruction is None:
        print('Reconstruction ground file missing for Dataset2: ' + path + d2_ground_reconstruction_path)

    ground_prediction1, ground_prediction2 = read_prediction_csv(osp.join(path,  d2_ground_prediction_path))
    if ground_prediction1 is None or ground_prediction2 is None:
        print('Prediction ground file missing for Dataset2: ' + path + d2_ground_prediction_path)

    ground_b = read_parameter_csv(osp.join(path,  d2_ground_param_path))
    if ground_b is None:
        print('Parameter ground file missing for Dataset2: ' + path + d2_ground_param_path)

    return ground_reconstruction, ground_prediction1, ground_prediction2, ground_b

def read_output_dataset3(path):
    output_reconstruction = read_reconstruction_csv(osp.join(path,  d3_output_reconstruction_path))
    if output_reconstruction is None:
        print('CPS reconstruction output file missing for Dataset3: ' + path + d3_output_reconstruction_path)

    output_prediction1, output_prediction2 = read_prediction_csv(osp.join(path,  d3_output_prediction_path))
    if output_prediction1 is None or output_prediction2 is None:
        print('CPS prediction output file missing for Dataset3: ' + path + d3_output_prediction_path)

    output_b = read_parameter_csv(osp.join(path,  d3_output_param_path))
    if output_b is None:
        print('CPS parameter output file missing for Dataset3: ' + path + d3_output_param_path)

    return output_reconstruction, output_prediction1, output_prediction2, output_b

def read_ground_dataset3(path):
    ground_reconstruction = read_reconstruction_csv(osp.join(path,  d3_ground_reconstruction_path))
    if ground_reconstruction is None:
        print('Reconstruction ground file missing for Dataset3: ' + path + d3_ground_reconstruction_path)

    ground_prediction1, ground_prediction2 = read_prediction_csv(osp.join(path,  d3_ground_prediction_path))
    if ground_prediction1 is None or ground_prediction2 is None:
        print('Prediction ground file missing for Dataset3: ' + path + d3_ground_prediction_path)

    ground_b = read_parameter_csv(osp.join(path,  d3_ground_param_path))
    if ground_b is None:
        print('Parameter ground file missing for Dataset3: ' + path + d3_ground_param_path)

    return ground_reconstruction, ground_prediction1, ground_prediction2, ground_b

################################# Evaluation Metric: Squared Difference ##################################

# computes the squared difference between two lists
def compute_squared_diff(output_b, ground_b):
    if(size(output_b)!=size(ground_b)):
        print('ERROR. Lists size do not match!')
        return ('NA')

    squared_diff = [(ob - gb)**2 for ob,gb in zip(output_b, ground_b)]
    return(sum(squared_diff))

def compare_dataset1(output_b, ground_b):
    print('Computing Parameter Difference...')
    return(compute_squared_diff(output_b, ground_b))

def compare_dataset(output_reconstruction, output_prediction1, output_prediction2, output_b, ground_reconstruction, ground_prediction1, ground_prediction2, ground_b):
    if output_reconstruction is None or ground_reconstruction is None:
        print('Skipping computing reconstruction difference(number.of.birds)')
        reconstruction_squared_diff = -1
    else:
        print('Computing Reconstruction Difference(number.of.birds)...')
        reconstruction_squared_diff = compute_squared_diff(output_reconstruction, ground_reconstruction)

    if output_prediction1 is None or ground_prediction1 is None:
        print('Skipping computing prediction difference(number.birds)')
        prediction1_squared_diff = -1
    else:
        print('Computing Prediction Difference(number.birds)...')
        prediction1_squared_diff = compute_squared_diff(output_prediction1, ground_prediction1)

    if output_prediction2 is None or ground_prediction2 is None:
        print('Skipping computing prediction difference(number.birds2)')
        prediction2_squared_diff = -1
    else:
        print('Computing Prediction Difference(number.birds2)...')
        prediction2_squared_diff = compute_squared_diff(output_prediction2, ground_prediction2)

    if output_b is None or ground_b is None:
        print('Skipping computing parameter difference')
        b_squared_diff = -1
    else:
        print('Computing Parameter Difference...')
        b_squared_diff = compute_squared_diff(output_b, ground_b)

    return reconstruction_squared_diff, prediction1_squared_diff, prediction2_squared_diff, b_squared_diff

#################################### Util #####################################

def read_cli():
    parser = argparse.ArgumentParser()

    #parser.add_argument("eval_result_file", help="name of the evaluation report file (to be generated)")
    parser.add_argument("cps_output_dir", help="output directory containing the program output (for each dataset in a subdirectory) to be evaluated against the ground truth")
    parser.add_argument("ground_truth_dir", help="ground truth directory containing the ground truth for each dataset in a subdirectory")
    parser.add_argument("eval_output_dir", help="evaluation output directory where the evaluation report files generated by this program need to be stored")
    parser.add_argument('--dataset', default = 'all', help='comma separated names of the datasets (d1,d2,d3) that need to be evaluated (default: all)')

    args = parser.parse_args()
    return args

def column_type(datasets_selected):
    col_type = 'd1'
    for d in datasets_selected:
        if((d == 'd2') or (d == 'd3')):
            col_type = 'all'
    return col_type

def print_column_headers(result_path,col_type):
    if(col_type == 'all'):
        data = pd.DataFrame([['Dataset','Evaluation Metric','Value','(Value.Prediction.Tonight)','(Value.Prediction.TomorrowNight)']])
    else:
        data = pd.DataFrame([['Dataset','Evaluation Metric','Value']])

    with open(result_path + '.csv', 'w') as f:
        data.to_csv(f, index = 0, header=False)

#################################### Evaluation #####################################

#### Evaluation: Dataset 1
def eval_dataset1(output_path,ground_path,result_path):
    print('Evaluating Outputs (Dataset1)...')

    if(osp.isfile(output_path + d1_output_param_path) == False):
        print('CPS output file missing for Dataset1: ' + output_path + d1_output_param_path)
        print('Skipping computing parameter difference for Dataset1.')
        b_squared_diff = -1
    elif(osp.isfile(ground_path + d1_ground_param_path) == False):
        print('Ground truth file missing for Dataset1: ' + ground_path + d1_ground_param_path)
        print('Skipping computing parameter difference for Dataset1.')
        b_squared_diff = -1
    else:
        # Read estimated parameters
        output_b = read_output_dataset1(output_path)
        #print(output_b)
        # Read ground truth parameters
        ground_b = read_ground_dataset1(ground_path)
        #print(ground_b)
        # Compute squared difference between the estimated parameters and the ground truth parameters
        b_squared_diff = compare_dataset1(output_b, ground_b)

    #print(squared_diff_dataset1)
    # Print the squared different in eval report line 1

    with open(result_path + '.txt', 'a') as output:
        output.writelines('Dataset 1 Parameters Squared Difference: '  + str(b_squared_diff) + '\n')

    #data = pd.DataFrame([['Dataset','Evaluation Metric','Value','(Value.Prediction.Tonight)','(Value.Prediction.TomorrowNight)'],['Dataset1', 'Parameters Squared Difference', b_squared_diff]])
    data = pd.DataFrame([['Dataset1', 'Parameters Squared Difference', b_squared_diff]])
    with open(result_path + '.csv', 'a') as f:
        data.to_csv(f, index = 0, header=False)

#### Evaluation: Dataset 2
def eval_dataset2(output_path,ground_path,result_path):
    print('Evaluating Outputs (Dataset2)...')
    output_reconstruction, output_prediction1, output_prediction2, output_b = read_output_dataset2(output_path)
    ground_reconstruction, ground_prediction1, ground_prediction2, ground_b = read_ground_dataset2(ground_path)
    reconstruction_squared_diff, prediction1_squared_diff, prediction2_squared_diff, b_squared_diff = compare_dataset(output_reconstruction, output_prediction1, output_prediction2, output_b, ground_reconstruction, ground_prediction1, ground_prediction2, ground_b)

    if((prediction1_squared_diff == 'NA') or (prediction2_squared_diff == 'NA')):
        prediction_squared_diff = 'NA'
    else:
        prediction_squared_diff = prediction1_squared_diff+prediction2_squared_diff

    with open(result_path + '.txt', 'a') as output:
        output.writelines('Dataset 2 Reconstruction Squared Difference: '  + str(reconstruction_squared_diff)+'\n')
        output.writelines('Dataset 2 Prediction Squared Difference: '  + str(prediction1_squared_diff) + ' ' + str(prediction2_squared_diff) + ' ' + str(prediction_squared_diff)+'\n')
        output.writelines('Dataset 2 Parameters Squared Difference: '  + str(b_squared_diff)+'\n')

    #print(reconstruction_squared_diff)
    #print(prediction1_squared_diff)
    #print(prediction2_squared_diff)
    #print(b_squared_diff)

    data = pd.DataFrame([['Dataset2', 'Reconstruction Squared Difference', reconstruction_squared_diff,'',''],
                         ['', 'Prediction Squared Difference', prediction_squared_diff,prediction1_squared_diff,prediction2_squared_diff],
                         ['', 'Parameters Squared Difference', b_squared_diff,'','']])
    with open(result_path + '.csv', 'a') as f:
        data.to_csv(f, index = 0, header=False)

#### Evaluation: Dataset 3
def eval_dataset3(output_path,ground_path,result_path):
    print('Evaluating Outputs (Dataset3)...')
    output_reconstruction, output_prediction1, output_prediction2, output_b = read_output_dataset3(output_path)
    ground_reconstruction, ground_prediction1, ground_prediction2, ground_b = read_ground_dataset3(ground_path)
    reconstruction_squared_diff, prediction1_squared_diff, prediction2_squared_diff, b_squared_diff = compare_dataset(output_reconstruction, output_prediction1, output_prediction2, output_b, ground_reconstruction, ground_prediction1, ground_prediction2, ground_b)

    if((prediction1_squared_diff == 'NA') or (prediction2_squared_diff == 'NA')):
        prediction_squared_diff = 'NA'
    else:
        prediction_squared_diff = prediction1_squared_diff+prediction2_squared_diff

    with open(result_path + '.txt', 'a') as output:
        output.writelines('Dataset 3 Reconstruction Squared Difference: '  + str(reconstruction_squared_diff)+'\n')
        #output.writelines('\nDataset 3 Reconstruction Squared Difference: %.2f'  %reconstruction_squared_diff)
        output.writelines('Dataset 3 Prediction Squared Difference: '  + str(prediction1_squared_diff) + ' ' + str(prediction2_squared_diff) + ' ' + str(prediction_squared_diff)+'\n')
        #output.writelines('\nDataset 3 Prediction Squared Difference: %.2f %.2f %.2f'  %(prediction1_squared_diff, prediction2_squared_diff, (prediction1_squared_diff+prediction2_squared_diff)))
        output.writelines('Dataset 3 Parameters Squared Difference: '  + str(b_squared_diff)+'\n')

    #print(reconstruction_squared_diff)
    #print(prediction1_squared_diff)
    #print(prediction2_squared_diff)
    #print(b_squared_diff)

    data = pd.DataFrame([['Dataset3', 'Reconstruction Squared Difference', reconstruction_squared_diff,'',''],
                         ['', 'Prediction Squared Difference', prediction_squared_diff,prediction1_squared_diff,prediction2_squared_diff],
                         ['', 'Parameters Squared Difference', b_squared_diff,'','']])
    with open(result_path + '.csv', 'a') as f:
        data.to_csv(f, index = 0, header=False)

#################################### Main #####################################
def main():
    args = read_cli()
    ground_path = args.ground_truth_dir
    output_path = args.cps_output_dir
    eval_output_dir = args.eval_output_dir
    datasets = args.dataset

    print('Birdcast Evaluation Script Version: ' + version)
    print('CPS Output Directory: ' + output_path)
    print('Ground Truth Directory: ' + ground_path)
    print('Evaluation Output Directory: ' + eval_output_dir)
    print('Evaluation on Datasets: ' + datasets)

    if(osp.isdir(output_path) == False):
        print('CPS output directory does not exist: ' + output_path)
        print('Exiting!')
        sys.exit(1)

    if(osp.isdir(ground_path) == False):
        print('Ground truth directory does not exist: ' + ground_path)
        print('Exiting!')
        sys.exit(1)

    timestr = time.strftime("%Y%m%d-%H%M%S")

    if((osp.isdir(eval_output_dir))== False):
        try:
            os.makedirs(eval_output_dir)
        except OSError:
            print('Cannot create the evaluation output directory: ' + eval_output_dir)
            print('Exiting!')
            sys.exit(1)

    result_path = osp.join(eval_output_dir,  'birdcast-eval-report-sv' + version + '-' + timestr)

    if(datasets == 'all'):
        print_column_headers(result_path,'all')
        eval_dataset1(output_path,ground_path,result_path)
        eval_dataset2(output_path,ground_path,result_path)
        eval_dataset3(output_path,ground_path,result_path)
    else:
        # separate out dataset options and sort them (e.g., d1, d2, d3)
        datasets_selected = datasets.split(',')
        datasets_selected.sort()

        # if we are evaluating only d1 then we do not need all the columns in the evaluation report csv
        # (e.g., Value.Prediction.Tonight, Value.Prediction.TomorrowNight)
        col_type = column_type(datasets_selected)    # determines what columns we need (for d1 only or for d2/d3)
        print_column_headers(result_path,col_type)   # prints the column names in the evaluation report csv

        for d in datasets_selected:
            if(d == 'd1'):
                eval_dataset1(output_path,ground_path,result_path)
            elif(d == 'd2'):
                eval_dataset2(output_path,ground_path,result_path)
            elif(d == 'd3'):
                eval_dataset3(output_path,ground_path,result_path)
            else: print('ERROR. Invalid Dataset ' + d + '. Skipping.')

    print('Generated evaluation report: ' + result_path +'.csv')

if __name__ == '__main__':
    main()
